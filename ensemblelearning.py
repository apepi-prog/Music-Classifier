# -*- coding: utf-8 -*-
"""EnsembleLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19A92IC0Qhaewl6etHl9kTAdX10Vdbd_s
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import csv
import keras
import pickle
from keras.models import Sequential
from keras.layers import Activation, Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import gc

#!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_adapte.csv
#!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_head.csv
#!wget http://dept-info.labri.fr/~hanna/ProjetClassif/test_openl3.pickle
#!wget http://dept-info.labri.fr/~hanna/ProjetClassif/train_openl3.pickle


head_features = 'features_head.csv'
train_clean = 'train.csv'
test_clean = 'test.csv'
adapt_features = 'features_adapte.csv'

# Nom des features
features = pd.read_csv(filepath_or_buffer=head_features, sep=",")

# Jointure features/tracks du dataset train
traingenre = pd.read_csv(filepath_or_buffer=train_clean, sep=",")
iter_csv = pd.read_csv(filepath_or_buffer=adapt_features, sep=",", iterator=True, chunksize=10000)
datatrain = pd.concat([chunk for chunk in iter_csv])

#Jointure des features/tracks du datastet test
test = pd.read_csv(filepath_or_buffer=test_clean, sep=",")
iter_csv = pd.read_csv(filepath_or_buffer=adapt_features, sep=",", iterator=True, chunksize=10000)
datatest = pd.concat([chunk for chunk in iter_csv])

data = pd.merge(traingenre, datatrain, on='track_id')

#data use to our model training and test
#each extract got his descriptors values (519)
x = data.drop('genre_id',axis=1).values

#genre id associated to extract (between 1 and 8)
Y = data['genre_id'].values

## files to predict 
to_predict = pd.merge(test, datatest, on='track_id').values

x_train, x_test, y_train, y_test = train_test_split(x, Y, test_size=0.10)

# model LGBM tree
num_iter = 50
model = lgb.LGBMClassifier(learning_rate=0.11, max_depth=50, num_leaves=100, num_iterations=num_iter)
model.fit(x_train, y_train, eval_set=[(x_test,y_test),(x_train,y_train)], eval_metric='logloss')

print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))
print('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))

#get predictions from lgbm tree
predictions_lgbm = model.predict_proba(to_predict)

del traingenre
del iter_csv
del datatrain
del test
del datatest
del data
del x
del Y
del x_train
del x_test
del y_train
del y_test
del to_predict
del model 
gc.collect()

#With 0.09 lr we got 62 % accuracy here (not on the predictions to do for real)
#0.1 lr, depth=6 and num_leaves=17 num_iter = 80 we got 60 % accuracy on the real tests

from keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

train_data = pickle.load(open('train_openl3.pickle', 'rb'))
train_genres = 'train.csv'

traingenre = pd.read_csv(filepath_or_buffer=train_genres, sep=",")
genres_train = {}
for i in range(len(traingenre)):
    genres_train[str(traingenre.iloc[i][0]).rjust(6, '0')] = traingenre.iloc[i][1]

id_train = [x[0][0:6] for x in train_data]
id_genre_train = [genres_train[g]-1 for g in id_train]
embed_train = [x[1] for x in train_data]

del id_train
del genres_train
del traingenre
del train_genres
del train_data
gc.collect()

#splitting our data (20% used for testing)
x_train, x_test, y_train, y_test = train_test_split(embed_train, id_genre_train, test_size=0.20)
del embed_train
del id_genre_train
gc.collect()

#Reshape for conv layers and one hot encoding y
x_train = np.array(x_train)
x_test = np.array(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

x_train = x_train.reshape(-1, 290, 512, 1)
x_test = x_test.reshape(-1, 290, 512, 1)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

#get predictions from our best model with OpenL3 data
checkpoint_filepath='my_best_model.hdf5'
network = Sequential()
network.add(Conv2D(32, (3, 3), padding="same", activation='relu'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2, 2)))
network.add(Conv2D(64, (3, 3), padding="same", activation='tanh'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2, 2)))
network.add(Conv2D(128, (3, 3), padding="same", activation='tanh'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2, 2)))
network.add(Conv2D(256, (3, 3), padding="same", activation='tanh'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2, 2)))
network.add(Conv2D(512, (3, 3), padding="same", activation='tanh'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2, 2)))
network.add(Dropout(0.4))
network.add(Flatten())
network.add(BatchNormalization())
network.add(Dense(512, activation='relu'))
network.add(BatchNormalization())
network.add(Dense(128, activation='tanh'))
network.add(BatchNormalization())
network.add(Dense(64, activation='tanh'))
network.add(BatchNormalization())
network.add(Dropout(0.4))
network.add(Dense(8, activation='softmax'))

#compile model 
#opt = keras.optimizers.Adam(learning_rate=0.003)
opt = keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07)
rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=1e-7)

#create callback
checkpoint_filepath = 'my_best_model_weights.hdf5'

checkpoint = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

network.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

epochs = 50
batch_size = 32
# fit the model 
network.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=[rlrop, checkpoint])

#reevaluate
network.load_weights(checkpoint_filepath)
_, accuracy = network.evaluate(x_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))

del x_train
del x_test
del y_train
del y_test
gc.collect()

#network.load_weights('my_best_model_weights.hdf5')
test_data = pickle.load(open('test_openl3.pickle', 'rb'))
test_data.sort()
embed_test = [x[1] for x in test_data]

del test_data

embed_test = np.array(embed_test)
embed_test = embed_test.reshape(-1, 290, 512, 1)
pred_network = network.predict(embed_test)

del network
del embed_test
gc.collect()

#LGBM using VGG
!wget http://dept-info.labri.fr/~hanna/ProjetClassif/train_vgg.joblib
!wget http://dept-info.labri.fr/~hanna/ProjetClassif/train_genres.joblib

!wget http://dept-info.labri.fr/~hanna/ProjetClassif/test_vgg.joblib
!wget http://dept-info.labri.fr/~hanna/ProjetClassif/test_ids.joblib

!wget http://dept-info.labri.fr/~hanna/ProjetClassif/test_id_vgg.pickle

import joblib

embed = joblib.load(open('train_vgg.joblib','rb'))
genres = joblib.load(open('train_genres.joblib','rb'))

embed = [x[0] for x in embed]

embed = np.array(embed)
genres = np.array(genres)

genres = [int(x[0])-1 for x in genres]

x_train, x_test, y_train, y_test = train_test_split(embed, genres)

del embed
del genres
gc.collect()

num_classes = 8
#flattening for lgbm
x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] *x_train.shape[2]))
x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] *x_test.shape[2]))

# model LGBM tree
model = lgb.LGBMClassifier(learning_rate=0.01, max_depth=70, num_leaves=300, num_iterations=50, early_stopping_rounds=10)
model.fit(x_train, y_train, eval_set=[(x_test,y_test),(x_train,y_train)], eval_metric='logloss')

print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))
print('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))

del x_train
del x_test
del y_train
del y_test
gc.collect()

data = pickle.load(open('test_id_vgg.pickle','rb'))
data.sort()
tmp = [(x[0],x[1]) for x in data if len(x[1]) == 31]
ids = [x[0] for x in tmp]
embed = [x[1] for x in tmp]

x_real_test = np.array(embed)
ids_real_test = np.array(ids)

x_real_test = x_real_test.reshape((x_real_test.shape[0], x_real_test.shape[1] *x_real_test.shape[2]))

pred_vgg = model.predict_proba(x_real_test)

del embed
del x_real_test
del tmp 
del model
gc.collect()

#data used for lgbm tree (classical with 512 values for each vectors)
test = pd.read_csv(filepath_or_buffer=test_clean, sep=",")
id_songs = [str(test.iloc[i][0]).rjust(6, '0') for i in range(len(test))]

#data used for cnn
test_data = pickle.load(open('test_openl3.pickle', 'rb'))
test_data.sort()
id_test = [x[0] for x in test_data] 

#adding missing values on openl3 predictions missing
for i in range(6):
  for i in range(len(id_test)):
    if (id_test[i] != id_songs[i]):
      id_test = np.insert(id_test, i, id_songs[i], axis=0)
      pred_network = np.insert(pred_network, i, predictions_lgbm[i], axis=0)
      break

print(pred_network.shape)

#adding missing values on vgg predictions missing
for i in range(6):
  for i in range(len(ids_real_test)):
    if (ids_real_test[i] != id_songs[i]):
      ids_real_test = np.insert(ids_real_test, i, id_songs[i], axis=0)
      pred_vgg = np.insert(pred_vgg, i, predictions_lgbm[i], axis=0)
      break

#finding the most reccurent pred
predictions = []
#for i in range(4008):
#  classes = [np.argmax(pred_vgg[i]), np.argmax(predictions_lgbm[i]), np.argmax(pred_network[i])]
#  for c in classes:
#    if (classes.count(c) > 1):
#      predictions.append(c+1)
#      break
#  if (len(predictions) < i + 1):
#    max_proba = max([np.max(pred_vgg[i]), np.max(predictions_lgbm[i]), np.max(pred_network[i])])
#    if (max_proba == np.max(pred_vgg[i])):
#      predictions.append(np.argmax(pred_vgg[i])+1)
#    if (max_proba == np.max(predictions_lgbm[i])):
#      predictions.append(np.argmax(predictions_lgbm[i]+1))
#    if (max_proba == np.max(pred_network[i])):
#      predictions.append(np.argmax(pred_network[i]+1))

#mean of predictions
for i in range(4008):
  mean_proba = np.array((pred_vgg[i] + predictions_lgbm[i] + pred_network[i]) / 3.0)
  predictions.append(np.argmax(mean_proba) + 1)

del test_data
del pred_vgg
del pred_network
del predictions_lgbm
gc.collect()

#Creating our result csv
with open('submisission_ensemble.csv', 'w', encoding='UTF8', newline='') as f:
    writer = csv.writer(f)
    
    # write the header
    header = ['track_id', 'genre_id']
    writer.writerow(header) 
    
    for i in range(len(id_songs)):
        id_genre = predictions[i]
        d = [id_songs[i], id_genre]
        ## write predictions here
        # data like this id_song, id_genre
        writer.writerow(d)
        
print("Predictions written in submisission_ensemble.csv")
#With lgbm trained on vector of 512 values (used before) and the CNN using OpenL3 output
# we are around 45% accuracy 

#Now combination of three models (one trained on OpenL3, one on Vgg, one on vector512)